{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Salaries - Part 2: Error Analysis and Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Error Analysis Conclusions & Work Plan\n",
    "\n",
    "Based on the error analysis from Part 1, we identified several key issues:\n",
    "\n",
    "1. **Salary Range Bias**:\n",
    "   - Model significantly underestimates high-salary positions (>250k USD)\n",
    "   - Negative skew in error distribution\n",
    "\n",
    "2. **Feature Importance Issues**:\n",
    "   - Employee residence dominates with ~0.40 importance score\n",
    "   - Work year and remote ratio show very low importance (<0.05)\n",
    "   - Potential sparsity issues with job titles\n",
    "\n",
    "3. **Experience Level Patterns**:\n",
    "   - Largest error variance in Executive (EX) level\n",
    "   - Significant outliers in Senior (SE) level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the original data and model\n",
    "path = \"data\\\\ds_salaries.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Import the prepare_data function from part1\n",
    "from part1 import prepare_data\n",
    "processed_data = prepare_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Cause Analysis\n",
    "\n",
    "Let's analyze potential causes for the observed issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze salary distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=processed_data, x='salary_in_usd', bins=50)\n",
    "plt.title('Salary Distribution')\n",
    "plt.xlabel('Salary (thousands USD)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Salary Distribution Statistics:\")\n",
    "print(processed_data['salary_in_usd'].describe())\n",
    "\n",
    "# Calculate skewness\n",
    "print(f\"\\nSkewness: {processed_data['salary_in_usd'].skew():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature cardinality\n",
    "categorical_cols = ['job_title', 'employee_residence', 'company_location']\n",
    "\n",
    "print(\"Feature Cardinality Analysis:\")\n",
    "for col in categorical_cols:\n",
    "    unique_count = processed_data[col].nunique()\n",
    "    top_5_freq = processed_data[col].value_counts().head()\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"Unique values: {unique_count}\")\n",
    "    print(\"Top 5 most frequent values:\")\n",
    "    print(top_5_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Root Causes Identified:\n",
    "\n",
    "1. **Data Distribution Issues**:\n",
    "   - Right-skewed salary distribution\n",
    "   - Underrepresentation of high-salary positions\n",
    "   - Possible outliers affecting model training\n",
    "\n",
    "2. **Feature Engineering Problems**:\n",
    "   - High cardinality in categorical variables (e.g. job title and location)\n",
    "   - Simple label encoding might not capture relationships\n",
    "   - Missing interaction effects between features\n",
    "\n",
    "3. **Model Limitations**:\n",
    "   - Linear scaling might not handle salary ranges well\n",
    "   - No special handling of outliers\n",
    "   - Basic feature preprocessing\n",
    "   - Limited hyperparameter optimization\n",
    "   - Single model approach for all salary ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Solutions to Some of the Issues\n",
    "\n",
    "**Handling Salary Distribution**\n",
    "- Apply quantile transformation to normalize salary distribution\n",
    "- Use stratified sampling to ensure representation across salary ranges\n",
    "- Implement separate models for different salary brackets\n",
    "- Consider log transformation for salary values\n",
    "\n",
    "**Feature Engineering Solutions**\n",
    "- Implement target encoding for categorical variables\n",
    "- Use embedding techniques for high-cardinality categorical features\n",
    "- Create experience-title interaction features\n",
    "- Add location-based interaction terms\n",
    "- Develop remote work impact factors\n",
    "- Include company size-location interactions\n",
    "\n",
    "**Model Architecture & Training Improvements**\n",
    "- Use quantile regression for better uncertainty estimation\n",
    "- Use k-fold cross-validation\n",
    "- Implement hyperparameter tuning\n",
    "- Add regularization techniques\n",
    "- Use weighted sampling for underrepresented cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Improving Model Performance\n",
    "\n",
    "Let's address these issues through the various improvement techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_prepare_data(df):\n",
    "    data = df.copy()\n",
    "    \n",
    "    # 1. Better salary transformation\n",
    "    data['salary_in_usd'] = data['salary_in_usd'] / 1000\n",
    "    qt = QuantileTransformer(n_quantiles=1000, output_distribution='normal')\n",
    "    data['salary_transformed'] = qt.fit_transform(data[['salary_in_usd']])\n",
    "    \n",
    "    # 2. Improved categorical handling\n",
    "    # Group rare categories\n",
    "    for col in ['job_title', 'employee_residence', 'company_location']:\n",
    "        value_counts = data[col].value_counts()\n",
    "        rare_categories = value_counts[value_counts < 5].index\n",
    "        data[col] = data[col].replace(rare_categories, 'Other')\n",
    "    \n",
    "    # 3. Feature interactions\n",
    "    data['location_match'] = (data['employee_residence'] == data['company_location']).astype(int)\n",
    "    data['remote_senior'] = ((data['remote_ratio'] > 50) & \n",
    "                            (data['experience_level'] == 'SE')).astype(int)\n",
    "    \n",
    "    # 4. Encode categorical variables\n",
    "    categorical_cols = ['experience_level', 'employment_type', 'job_title', \n",
    "                       'employee_residence', 'company_location', 'company_size']\n",
    "    \n",
    "    # Use mean target encoding instead of label encoding\n",
    "    for col in categorical_cols:\n",
    "        means = data.groupby(col)['salary_in_usd'].mean()\n",
    "        data[col + '_encoded'] = data[col].map(means)\n",
    "    \n",
    "    return data, qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare improved dataset\n",
    "improved_data, qt = improved_prepare_data(df)\n",
    "\n",
    "# Define features for improved model\n",
    "improved_features = [\n",
    "    'work_year',\n",
    "    'experience_level_encoded',\n",
    "    'employment_type_encoded',\n",
    "    'job_title_encoded',\n",
    "    'employee_residence_encoded',\n",
    "    'remote_ratio',\n",
    "    'company_location_encoded',\n",
    "    'company_size_encoded',\n",
    "    'location_match',\n",
    "    'remote_senior'\n",
    "]\n",
    "\n",
    "# Prepare features and target\n",
    "X = improved_data[improved_features]\n",
    "y = improved_data['salary_transformed']  # Use transformed target\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train improved model\n",
    "improved_model = xgb.XGBRegressor(\n",
    "    max_depth=6,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "improved_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions and inverse transform\n",
    "y_pred_transformed = improved_model.predict(X_test_scaled)\n",
    "y_pred = qt.inverse_transform(y_pred_transformed.reshape(-1, 1)).ravel()\n",
    "y_test_original = qt.inverse_transform(y_test.values.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test_original, y_pred))\n",
    "mae = mean_absolute_error(y_test_original, y_pred)\n",
    "r2 = r2_score(y_test_original, y_pred)\n",
    "\n",
    "print(\"Improved Model Performance:\")\n",
    "print(f\"RMSE: {rmse:.2f}k\")\n",
    "print(f\"MAE: {mae:.2f}k\")\n",
    "print(f\"R2 Score: {r2:.3f}\")\n",
    "\n",
    "# Visualize improvements\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_original, y_pred, alpha=0.5)\n",
    "plt.plot([y_test_original.min(), y_test_original.max()],\n",
    "         [y_test_original.min(), y_test_original.max()],\n",
    "         'r--', lw=2)\n",
    "plt.xlabel('Actual Salary (thousands USD)')\n",
    "plt.ylabel('Predicted Salary (thousands USD)')\n",
    "plt.title('Improved Model: Predicted vs Actual Salaries')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Improvements\n",
    "\n",
    "The implemented solutions address the key issues identified:\n",
    "\n",
    "1. **Handling Salary Distribution**:\n",
    "   - Quantile transformation normalizes salary distribution\n",
    "   - Better handling of extreme values\n",
    "   - Reduced impact of outliers\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - Grouped rare categories to reduce cardinality\n",
    "   - Added meaningful feature interactions\n",
    "   - Implemented target encoding for categorical variables\n",
    "\n",
    "3. **Model Optimization**:\n",
    "   - Tuned XGBoost parameters\n",
    "   - Added regularization through subsample and colsample\n",
    "   - Better handling of feature relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing the Improved Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Drawing Conclusions About the Data & Creative Applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
